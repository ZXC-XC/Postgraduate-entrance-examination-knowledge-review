- [第7讲 大数定律与中心极限定理](#第7讲-大数定律与中心极限定理)
  - [知识结构](#知识结构)
    - [依概率收敛](#依概率收敛)
    - [大数定律](#大数定律)
    - [中心极限定理](#中心极限定理)
  - [一、依概率收敛](#一依概率收敛)
  - [二、大数定律](#二大数定律)
  - [三、中心极限定理](#三中心极限定理)


# 第7讲 大数定律与中心极限定理

## 知识结构

### 依概率收敛  
$$
\lim_{n\to\infty} P(|X_n - X|\ge \varepsilon)=0 \qquad
\lim_{n\to\infty} P(|X_n - X|< \varepsilon)=1
$$

### 大数定律

**切比雪夫大数定律**  
$$
\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{P} \frac{1}{n}\sum_{i=1}^{n} EX_i
$$

**伯努利大数定律** 
$$
\lim_{n\to\infty} P\!\left(\left|\frac{\mu_n}{n}-p\right|<\varepsilon\right)=1
$$

**辛钦大数定律** 
$$
\lim_{n\to\infty} P\!\left(\left|\frac{1}{n}\sum_{i=1}^{n} X_i - \mu\right|<\varepsilon\right)=1
$$

**考结论**  
$$
\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{P} E\!\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right)
$$

### 中心极限定理

**列维–林德伯格定理**  
$$
\lim_{n\to\infty} P\!\left(\frac{\sum_{i=1}^n X_i - n\mu}{\sqrt{n}\,\sigma} \le x \right)
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x} e^{-t^{2}/2}\,dt = \Phi(x)
$$

**棣莫弗–拉普拉斯定理**  
$$
\lim_{n\to\infty} P\!\left\{\frac{Y_n - np}{\sqrt{np(1-p)}} \le x \right\}
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x} e^{-t^{2}/2}\,dt
= \Phi(x)
$$

**考结论**  
$$
\lim_{n\to\infty} P\!\left(\frac{\sum_{i=1}^n X_i - n\mu}{\sqrt{n}\,\sigma} \le x \right)
= \Phi(x)
$$

## 一、依概率收敛

设随机变量 $X$ 与随机变量序列 $\{X_n, n=1,2,3,\dots\}$，如果对任意的 $\varepsilon>0$，有  
$$
\lim_{n\to\infty} P(|X_n - X|\ge \varepsilon)=0 \quad \text{或} \quad \lim_{n\to\infty} P(|X_n - X|< \varepsilon)=1,
$$
则称随机变量序列 $\{X_n\}$ 依概率收敛于随机变量 $X$，记为  
$\lim_{n\to\infty} X_n = X\ (P)$ 或 $X_n \xrightarrow{P} X \ (n\to\infty)$。

**[注]** 
(1) 以上定义中将随机变量 $X$ 写成数 $a$ 也成立。

(2) 设 $X_n \xrightarrow{P} X$，$Y_n \xrightarrow{P} Y$，$g(x,y)$ 是二元连续函数，则 $g(X_n,Y_n) \xrightarrow{P} g(X,Y)$。一般地，对 $m$ 元连续函数 $g(x_1,x_2,\dots,x_m)$，上述结论亦成立。

(3) 在讨论未知参数估计量是否具有一致性（相合性）时，常常要用到依概率收敛这一性质和大数定律。

## 二、大数定律 

**1. 切比雪夫大数定律**  
假设 $\{X_i (i=1,2,\ldots)\}$ 为相互独立的随机变量列，如果方差 $D(X_i)\ (i\ge 1)$ 有上界且一致有界，即存在常数 $C$，使 $D X_i \le C$ 对一切 $i \ge 1$ 均成立，则 $\{X_i\}$ 服从大数定律：  
$$
\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{P} \frac{1}{n}\sum_{i=1}^{n} EX_i.
$$

**2. 伯努利大数定律**  
假设 $\mu_n$ 是 $n$ 重伯努利试验中事件 $A$ 发生的次数，在每次试验中事件 $A$ 发生的概率为 $p(0<p<1)$，则 $\frac{\mu_n}{n}\xrightarrow{P} p$，即对任意 $\varepsilon>0$，有  
$$
\lim_{n\to\infty} P\!\left(\left|\frac{\mu_n}{n}-p\right|<\varepsilon\right)=1.
$$

[注]在数理统计中，将 $(x_1,x_2,\ldots,x_n)$ 为总体样本 $(X_1,X_2,\ldots,X_n)$ 的一个观测值，按大小顺序排列为 $x_{(1)}\le x_{(2)}\le \cdots \le x_{(n)}$，对任意实数 $x$，称  
$$
F_n(x)=\frac{x_1,x_2,\ldots,x_n\ \text{中小于等于} x \ \text{的样本值个数}}{n}
$$
即  
$$
F_n(x)=
\begin{cases}
0, & x < x_{(1)},\$$4pt]
\frac{k}{n}, & x_{(k)} \le x < x_{(k+1)} \ (k=1,2,\ldots,n-1),\$$6pt]
1, & x \ge x_{(n)},
\end{cases}
$$
为样本 $(X_1,X_2,\ldots,X_n)$ 的经验分布函数。

事实上，$F_n(x)$ 表示事件 $\{X \le x\}$ 在 $n$ 次试验中出现的频率，而 $P\{X \le x\}=F(x)$ 是事件 $\{X \le x\}$ 出现的概率。由伯努利大数定律（即频率收敛于概率）可知，当 $n$ 充分大时，$F_n(x)$ 可作为分布函数 $F(x)$ 的一个近似，$n$ 越大，估计效果越好。
  

**3. 辛钦大数定律**  
若 $\{X_i (i=1,2,\ldots)\}$ 是独立同分布的随机变量列，如果数学期望 $EX_i=\mu (i=1,2,\ldots)$ 均存在，则  
$$
\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{P} \mu,
$$
即对任意 $\varepsilon>0$，有  
$$
\lim_{n\to\infty} P\!\left(\left|\frac{1}{n}\sum_{i=1}^{n} X_i - \mu\right|<\varepsilon\right)=1.
$$

**4. 考结论**  
在满足一定条件时，大数定律都存在共同的一个结论，即  
$$
\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{P} E\!\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right).
$$

## 三、中心极限定理

**1. 列维－林德伯格定理**  
假设 $\{X_i\}$ 是独立同分布的随机变量列，如果 $EX_i=\mu,\ DX_i=\sigma^{2}>0\ (i=1,2,\dots)$ 存在，则 $\{X_i\}$ 服从中心极限定理，即对任意实数$x$,有  
$$
\lim_{n\to\infty} P\!\left\{ \frac{\sum_{i=1}^{n} X_i - n\mu}{\sqrt{n}\,\sigma} \le x \right\}
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x} e^{-t^{2}/2}\,dt = \Phi(x).
$$

**[注] (1)** 定理中三个条件 **独立、同分布、期望与方差存在**，缺一不可。  
**(2)** 只要 $X_n$ 满足定理条件，那么当 $n$ 很大时，独立同分布随机变量的和 $\sum_{i=1}^{n} X_i$ 近似服从正态分布 $N(n\mu,n\sigma^{2})$，由此可得，当 $n$ 足够大，有  
$$
P\{a<\sum_{i=1}^{n} X_i < b\} \approx \Phi\!\left(\frac{b-n\mu}{\sqrt{n}\,\sigma}\right) - \Phi\!\left(\frac{a-n\mu}{\sqrt{n}\,\sigma}\right).
$$

**2. 棣莫弗－拉普拉斯定理**  
假设随机变量 $Y_n \sim B(n,p)(0<p<1,\ n\ge 1)$，则对任意实数$x$：有  
$$
\lim_{n\to\infty} P\!\left\{ \frac{Y_n - np}{\sqrt{np(1-p)}} \le x \right\}
= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-t^{2}/2}\,dt = \Phi(x).
$$

**[注] (1)** 若把 $X_i \sim B(1,p)(0<p<1)$，则 $X_i \sim \begin{pmatrix}1 \\ 0\end{pmatrix} \begin{pmatrix}p \\ 1-p\end{pmatrix}$ 互相独立，则  
$$
Y_n = \sum_{i=1}^{n} X_i \sim B(n,p).
$$  
由列维－林德伯格定理直接推出棣莫弗－拉普拉斯定理。  
**(2) 二项分布概率计算的三种方法。**

设 $X \sim B(n,p)$。

① 当 $n$ 不太大时 $(n \le 10)$，直接计算  
$$
P\{X=k\}=C_n^{k} p^{k}(1-p)^{n-k},\quad k=0,1,\dots,n;
$$

② 当 $n$ 较大且 $p$ 较小时 $(n>10,\ p<0.1)$，$\lambda = np$ 适中，根据泊松定理有近似公式  
$$
P\{X=k\}=C_n^{k} p^{k}(1-p)^{n-k} \approx \frac{\lambda^{k}}{k!} e^{-\lambda},\quad k=0,1,\dots,n;
$$

③ 当 $n$ 较大而 $p$ 不太大时 $(p<0.1, np \ge 10)$，根据中心极限定理，有近似公式  

$P\{a < X < b\} \approx \Phi\!\left(\dfrac{b-np}{\sqrt{np(1-p)}}\right)-\Phi\!\left(\dfrac{a-np}{\sqrt{np(1-p)}}\right)$.

**3. 考结论**

设 $X_i$ 独立同分布于某分布，期望、方差均存在，则当 $n \to \infty$ 时，$\sum_{i=1}^{n} X_i$ 近似服从正态分布。即不论 
$$
X_i \overset{iid}{\sim} F(\mu,\sigma^{2}),\quad \mu=E X_i,\ \sigma^{2}=D X_i
\ \Rightarrow\ 
\sum_{i=1}^{n} X_i \overset{n\to\infty}{\sim}  N(n\mu,n\sigma^{2})
\ \Rightarrow\ 
\frac{\sum_{i=1}^{n} X_i - n\mu}{\sqrt{n}\,\sigma} 
\overset{n\to\infty}{\sim}  N(0,1),
$$

即，
$$
\lim_{n\to\infty} P\!\left\{\frac{\sum_{i=1}^{n} X_i - n\mu}{\sqrt{n}\,\sigma} \le x \right\} = \Phi(x).
$$